{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luSS6e7lSL0H"
      },
      "source": [
        "# U-Net for cell segmentation\n",
        "---\n",
        "## Introduction\n",
        "This is a notebook that shows how to design and train a [U-Net](https://en.wikipedia.org/wiki/U-Net)-like network to segment cells in Phase Contrast Microscopy images using Keras and Tensorflow. The aim is to train the network using original phase contrast microscopy images as input, and one label image per category (background, foreground and contours) as output.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=18-cP68ms6vg42V2EHzheTatuJLQ0sJJz\" width=\"750\">\n",
        "</figure>\n",
        "\n",
        "These notebook is based on the previous development of the following authors for the [NEUBIAS text book 2021](https://github.com/NEUBIAS/neubias-springer-book-2021).\n",
        "\n",
        "**Authors**: **Authors**: [Estibaliz Gómez-de-Mariscal](https://henriqueslab.github.io/team/2021-10-01-EGdM/), [Daniel Franco-Barranco](https://danifranco.github.io), [Arrate Muñoz-Barrutia](https://image.hggm.es/es/arrate-munoz) and [Ignacio Arganda-Carreras](https://sites.google.com/site/iargandacarreras/).\n",
        "\n",
        "## Data\n",
        "The image data used in the notebook was provided by Dr. T. Becker. Fraunhofer Institution for Marine Biotechnology, Lübeck, Germany to the [Cell Tracking Challenge](http://celltrackingchallenge.net/). The acquisition details can be found [here](http://celltrackingchallenge.net/2d-datasets/#bg-showmore-hidden-5ea69464435099020168051).\n",
        "The training data set consists of 2D images of pancreatic stem cells on a polystyrene substrate. The field of view measures 1152 x 922 microns approx., with a resolution of 1.6 x 1.6 um/pixel. In this notebook we will use the gold reference tracking annotations (ST) of the training data (frames 150-250 of both videos). The annotations were binarized for this use-case. Find the data adapted for this notebook [here](https://github.com/esgomezm/NEUBIAS_chapter_DL_2020/releases/download/1/data4notebooks.zip): \n",
        "\n",
        "Frames of sequence 01 were used as training data and frames of the sequence 02 were used validation (frames 150, 140, 150, ..., 250) and test data (frames 151, 152, ..., 248, 249).\n",
        "\n",
        "The data is organized in different folders as follows:\n",
        "\n",
        "```\n",
        "./\n",
        "    |-- train_input\n",
        "    |    |      t150.tif\n",
        "    |    |      ...\n",
        "    |-- train_binary_masks\n",
        "    |    |      man_seg150.tif\n",
        "    |    |        ...\n",
        "    |-- train_contours\n",
        "    |    |      man_seg150.png\n",
        "    |    |        ...\n",
        "    |-- validation_input\n",
        "    |    |      t150.tif\n",
        "    |    |      ...\n",
        "    |-- validation_binary_masks\n",
        "    |    |      man_seg150.tif\n",
        "    |    |        ...\n",
        "    |-- validation_contours\n",
        "    |    |      man_seg150.png\n",
        "    |    |        ...\n",
        "    |-- test_input\n",
        "    |    |      t151.tif\n",
        "    |    |      ...\n",
        "    |-- test_binary_masks\n",
        "    |    |      man_seg151.tif\n",
        "    |    |        ...\n",
        "    |-- test_contours\n",
        "    |    |      man_seg151.png\n",
        "    |    |        ...\n",
        "```\n",
        "\n",
        "\n",
        "## Network arquitecture:\n",
        "\n",
        "We will train the standard encoder-decoder called U-Net.\n",
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=14zaw3eomx_2F__8STpzPaedi_r-gbZYR\" width=\"750\">\n",
        "</figure>\n",
        "\n",
        "This network was introduced by\n",
        "\n",
        "- U-Net: Convolutional Networks for Biomedical Image Segmentation by Ronneberger et al. published on arXiv in 2015 (https://arxiv.org/abs/1505.04597)\n",
        "\n",
        "and\n",
        "\n",
        "- U-Net: deep learning for cell counting, detection, and morphometry by Thorsten Falk et al. in Nature Methods 2019 (https://www.nature.com/articles/s41592-018-0261-2) And source code found in: https://github.com/zhixuhao/unet by Zhixuhao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSckjyNSp6D9"
      },
      "source": [
        "## Getting started\n",
        "Download the ZIP file with the image data and unzip it in Google's content. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcJn0lZtEq9z"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "# Download file with image data\n",
        "!wget 'https://github.com/esgomezm/NEUBIAS_chapter_DL_2020/releases/download/1/data4notebooks.zip'\n",
        "path2zip= 'data4notebooks.zip'\n",
        "\n",
        "# Extract locally\n",
        "with zipfile.ZipFile(path2zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK-GgwqbrkI2"
      },
      "source": [
        "We should be able to read the list of **101 training images**, together with their corresponding binary masks and cell contours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIuhK8x0-wBC"
      },
      "source": [
        "import os\n",
        "# Path to the training images\n",
        "train_input_path = '/content/dataset/train_input'\n",
        "train_masks_path = '/content/dataset/train_binary_masks'\n",
        "train_contours_path = '/content/dataset/train_contours'\n",
        "# Read the list of file names and sort them to have a match between images and masks\n",
        "train_input_filenames = [x for x in os.listdir( train_input_path ) if x.endswith(\".tif\")]\n",
        "train_input_filenames.sort()\n",
        "train_masks_filenames = [x for x in os.listdir( train_masks_path ) if x.endswith(\".tif\")]\n",
        "train_masks_filenames.sort()\n",
        "train_contours_filenames = [x for x in os.listdir( train_contours_path ) if x.endswith(\".png\")]\n",
        "train_contours_filenames.sort()\n",
        "\n",
        "print( 'Number of training input images: ' + str( len(train_input_filenames)) )\n",
        "print( 'Number of training binary mask images: ' + str( len(train_masks_filenames)) )\n",
        "print( 'Number of training contour images: ' + str( len(train_contours_filenames)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDfRdihFrw74"
      },
      "source": [
        "Next, we read all those images into memory and display one with its corresponding labels (masks and countours)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hckMCAdLEn8W"
      },
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Read training images (input, mask and contours)\n",
        "train_img = [cv2.imread(os.path.join(train_input_path, x),\n",
        "                        cv2.IMREAD_ANYDEPTH) for x in train_input_filenames ]\n",
        "train_masks = [cv2.imread(os.path.join(train_masks_path, x),\n",
        "                          cv2.IMREAD_ANYDEPTH)>0 for x in train_masks_filenames ]\n",
        "train_contours = [cv2.imread(os.path.join(train_contours_path, x),\n",
        "                             cv2.IMREAD_ANYDEPTH)>0 for x in train_contours_filenames ]\n",
        "\n",
        "# display the image\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow( train_img[0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Full-size training image' )\n",
        "# its \"mask\"\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow( train_masks[0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Binary mask' )\n",
        "# and cell contours\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow( train_contours[0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Object contour' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrZJHL0AkYzz"
      },
      "source": [
        "To facilitate their processing, we concatenate the binary masks and the contours to get one array with the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv6cer-0klHJ"
      },
      "source": [
        "import numpy as np\n",
        "train_output = [np.transpose(np.array([train_masks[i], train_contours[i]]),\n",
        "                             [1,2,0]) for i in range(len(train_masks))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPZVNfxDgubX"
      },
      "source": [
        "Next, we read the 11 images that will be used for validation and inspect some of them visually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUkhuAHwgrqR"
      },
      "source": [
        "# Path to the validation images\n",
        "val_input_path = '/content/dataset/validation_input'\n",
        "val_masks_path ='/content/dataset/validation_binary_masks'\n",
        "val_contours_path = '/content/dataset/validation_contours'\n",
        "\n",
        "# Read the list of file names and sort them to have a match between images and masks\n",
        "val_input_filenames = [x for x in os.listdir(val_input_path ) if x.endswith(\".tif\")]\n",
        "val_input_filenames.sort()\n",
        "val_masks_filenames = [x for x in os.listdir(val_masks_path ) if x.endswith(\".tif\")]\n",
        "val_masks_filenames.sort()\n",
        "val_contours_filenames = [x for x in os.listdir(val_contours_path ) if x.endswith(\".png\")]\n",
        "val_contours_filenames.sort()\n",
        "\n",
        "print( 'Images loaded: ' + str( len(val_input_filenames)) )\n",
        "\n",
        "# Read training images\n",
        "val_img = [cv2.imread(os.path.join(val_input_path, x), cv2.IMREAD_ANYDEPTH) for x in val_input_filenames ]\n",
        "val_masks = [cv2.imread(os.path.join(val_masks_path, x), cv2.IMREAD_ANYDEPTH)>0 for x in val_masks_filenames ]\n",
        "val_contours = [cv2.imread(os.path.join(val_contours_path, x), cv2.IMREAD_ANYDEPTH)>0 for x in val_contours_filenames ]\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(val_img[0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Full-size training image' )\n",
        "# Its \"mask\"\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(val_masks[0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Binary mask' )\n",
        "# And cell contours\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(val_contours[0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Object contour' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7craIKDky78"
      },
      "source": [
        "Again, to facilitate their processing, we concatenate the binary masks and the contours to get one array with the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1ea-0zskuNK"
      },
      "source": [
        "# concatenate binary masks and contours\n",
        "val_output = [np.transpose(np.array([val_masks[i],val_contours[i]]),\n",
        "                           [1,2,0]) for i in range(len(val_masks))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R8Izj7YsLe3"
      },
      "source": [
        "## Preparing the training data\n",
        "Now, we are going to create the training set by randomly cropping the input images (and their corresponding labels) into **patches of 256 x 256 pixels**.\n",
        "\n",
        "All the images entering the network need to be normalized so their intensities are all of them in the range [0,1]. For this, we will create a function that normalizes the intensities values. \n",
        "\n",
        "<font color='blue'>**Exercises**:</font> \n",
        "1. Program a function that takes an input image `x` with it's corresponding ground tuth `y` and crops random patches of a given shape.\n",
        "\n",
        "2. Program a function that performs the normalization of the intensity values of an input image `x` using percentiles with `np.percentile`.\n",
        "\n",
        "Notice that the ground truth has also its intensities scaled between 0.0 and 1.0.\n",
        "\n",
        "<font color='blue'>**Questions for discussion**.</font> \n",
        "- In this exercise we are loading the entire dataset in memory. Do you think this is an optimal solution for a real application? Specially when we have more than 100 images.\n",
        "- Why do you think that we need to crop patches?\n",
        "- How do you decide the size of the pathces?\n",
        "- Why do we need to normalize the intensity values of the input image? Can you think about would it be done for a different image modality (RGB) or for example, a whole slide images? Moreover, how can this affect the performance of the network with new images that have a different size or for which we did not crop patches?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random patches of 256x256 pixels (of corresponding input and labels,\n",
        "# i.e. mask and countour image)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# We define a method to create an arbitrary number of random crops of\n",
        "# a given size\n",
        "def create_random_patches( imgs, masks, num_patches, shape ):\n",
        "    ''' Create a list of images patches out of a list of images\n",
        "    Args:\n",
        "        imgs (list): input images.\n",
        "        masks (list): binary masks (output images) corresponding to imgs.\n",
        "        num_patches (int): number of patches for each image.\n",
        "        shape (2D array): size of the patches. Example: [256, 256].\n",
        "        \n",
        "    Returns:\n",
        "        list of image patches and patches of corresponding labels (background,\n",
        "        foreground and contours)\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    return input_patches, output_patches\n",
        "\n",
        "\n",
        "# Use the method to create six 256x256 pixel-sized patches per image\n",
        "train_input_patches, train_output_patches = create_random_patches( train_img, \n",
        "                                                                  train_output, \n",
        "                                                                  6, [256,256])\n",
        "\n",
        "# Normalization functions from Martin Weigert\n",
        "def normalizePercentile(x, pmin=1, pmax=99.8, axis=None, eps=1e-20):\n",
        "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
        "    \"\"\"Percentile-based image normalization.\"\"\"\n",
        "\n",
        "    return x\n",
        "\n",
        "X_train = [normalizePercentile(x) for x in train_input_patches] # normalize between 0 and 1"
      ],
      "metadata": {
        "id": "WYmxsOS-875e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To follow Tensorflow standards, the input and output of the network have to be reshaped to 256 x 256 x 1. Therefore, the array containing the input images should have shape `[n, 256, 256, 1]` where `n` is the number of patches that will be used for the training. \n"
      ],
      "metadata": {
        "id": "U2wIZabm8q6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In X_train we will store the input images\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "print('There are {} patches to train the network'.format(len(X_train)))"
      ],
      "metadata": {
        "id": "7zm1AADH9GmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to predict three labels (background, foreground and cell-contours). One way to provide the data is with one hot encoding (e.g., \n",
        "\n",
        "```\n",
        "[1, 2, 3, 2] --> [[1, 0, 0, 0],\n",
        "                  [0, 1, 0, 1],\n",
        "                  [0, 0, 1, 0]] \n",
        "```\n",
        "\n",
        "<font color='blue'>**Exercise**.</font> Create the ground truth array `Y_train` with a one hot encoding format. Note that you will need as many matrices as labels (i.e., 3) for each image."
      ],
      "metadata": {
        "id": "QXeRPKho9P4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In Y_train we will store the target labels for the network in a one-hot\n",
        "# fashion, so first channel for background, second for foreground (cells) and\n",
        "# third one for cell boundaries (contours)\n",
        "\n"
      ],
      "metadata": {
        "id": "REisNUe69JJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the results"
      ],
      "metadata": {
        "id": "GkWoS78u9L7_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dQj-_hDFEv4"
      },
      "source": [
        "# Display one patch\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.imshow( X_train[0,:,:,0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Training patch' )\n",
        "# Background class\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.imshow( Y_train[0,:,:,0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Binary patch (ground truth)' )\n",
        "# Foreground class\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.imshow( Y_train[0,:,:,1], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Binary patch (ground truth)' )\n",
        "# Object contours\n",
        "plt.subplot(1, 5, 4)\n",
        "plt.imshow( Y_train[0,:,:,2], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Object contours patch (ground truth)' )\n",
        "# Reversed one hot representation\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.imshow( np.argmax(Y_train[0], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Unique labelling of each pixel (ground truth)' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSDrFIzXVEF9"
      },
      "source": [
        "And now we do the same for the validation images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBYhGigRVGbx"
      },
      "source": [
        "# We first create the validation patches\n",
        "val_input_patches, val_output_patches = create_random_patches( val_img, val_output,\n",
        "                                                              6, [256,256])\n",
        "\n",
        "# In X_val we will store the input images\n",
        "X_val = [normalizePercentile(x) for x in val_input_patches] # normalize between 0 and 1\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "\n",
        "print('There are {} patches to validate the network'.format(len(X_val)))\n",
        "\n",
        "# In Y_val we will store the one-hot respresentation of the labels\n",
        "Y_val = [np.stack([1 - x[:,:,0] - x[:,:,1], x[:,:,0], x[:,:,1]],\n",
        "                  axis=-1) for x in val_output_patches ]\n",
        "Y_val = np.asarray(Y_val)\n",
        "\n",
        "# Display one patch\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.imshow( X_val[0,:,:,0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Validation patch' )\n",
        "# Background class\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.imshow( Y_val[0,:,:,0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Binary patch (ground truth)' )\n",
        "# Foreground class\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.imshow( Y_val[0,:,:,1], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Binary patch (ground truth)' )\n",
        "# Object contours\n",
        "plt.subplot(1, 5, 4)\n",
        "plt.imshow( Y_val[0,:,:,2], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Object contours patch (ground truth)' )\n",
        "# Reversed one hot representation\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.imshow( np.argmax(Y_val[0], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Unique labelling of each pixel (ground truth)' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRNINwivV3km"
      },
      "source": [
        "## Custom segmentation metric: Jaccard index\n",
        "We define as well the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) (also known as Intersection over the Union or IoU) to monitor de segmentation performance.\n",
        "\n",
        "**Note**: by default we skip the background label in the calculation since most of the pixels are background and the metric value would be artificially high otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnDDhxpxoSkN"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def jaccard_index( y_true, y_pred, skip_background=True ):\n",
        "    ''' Define Jaccard index for multiple labels.\n",
        "        Args:\n",
        "            y_true (tensor): ground truth masks.\n",
        "            y_pred (tensor): predicted masks.\n",
        "            skip_background (bool, optional): skip 0-label from calculation.\n",
        "        Return:\n",
        "            jac (tensor): Jaccard index value\n",
        "    '''\n",
        "    # We read the number of classes from the last dimension of the true labels\n",
        "    num_classes = tf.shape(y_true)[-1]\n",
        "    # One_hot representation of predicted segmentation after argmax\n",
        "    y_pred_ = tf.one_hot(tf.math.argmax(y_pred, axis=-1), num_classes)\n",
        "    y_pred_ = tf.cast(y_pred_, dtype=tf.int32)\n",
        "    # y_true is already one-hot encoded\n",
        "    y_true_ = tf.cast(y_true, dtype=tf.int32)\n",
        "    # Skip background pixels from the Jaccard index calculation\n",
        "    if skip_background:\n",
        "      y_true_ = y_true_[...,1:]\n",
        "      y_pred_ = y_pred_[...,1:]\n",
        "\n",
        "    TP = tf.math.count_nonzero(y_pred_ * y_true_)\n",
        "    FP = tf.math.count_nonzero(y_pred_ * (y_true_ - 1))\n",
        "    FN = tf.math.count_nonzero((y_pred_ - 1) * y_true_)\n",
        "\n",
        "    jac = tf.cond(tf.greater((TP + FP + FN), 0), lambda: TP / (TP + FP + FN),\n",
        "                  lambda: tf.cast(0.000, dtype='float64'))\n",
        "\n",
        "    return jac"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "let4jXdVtdQs"
      },
      "source": [
        "## Network definition\n",
        "Next, we define our U-Net-like network, with 3 resolution levels in the contracting path, a bottleneck, and 3 resolution levels in the expanding path. \n",
        "\n",
        "<font color='blue'>**Exercise**:</font> Try identifying each of the layers in the diagram of the network with the code lines that define the network below. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3O3hx36FxwF"
      },
      "source": [
        "# Create U-Net for segmentation\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, UpSampling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# We leave the height and width of the input image as \"None\" so the network can\n",
        "# later be used on images of any size.\n",
        "inputs = Input((None, None, 1))\n",
        "\n",
        "# Contracting path\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\n",
        "c1 = Dropout(0.1) (c1)\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "p1 = AveragePooling2D((2, 2)) (c1)\n",
        "\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "c2 = Dropout(0.2) (c2)\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "p2 = AveragePooling2D((2, 2)) (c2)\n",
        "\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "c3 = Dropout(0.3) (c3)\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "p3 = AveragePooling2D((2, 2)) (c3)\n",
        "\n",
        "# Bottleneck\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "c4 = Dropout(0.4) (c4)\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "\n",
        "# Expanding path\n",
        "u5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c4)\n",
        "u5 = concatenate([u5, c3])\n",
        "c5 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u5)\n",
        "c5 = Dropout(0.3) (c5)\n",
        "c5 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "\n",
        "u6 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "u6 = concatenate([u6, c2])\n",
        "c6 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "c6 = Dropout(0.2) (c6)\n",
        "c6 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "\n",
        "u7 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "u7 = concatenate([u7, c1], axis=3)\n",
        "c7 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "c7 = Dropout(0.1) (c7)\n",
        "c7 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "\n",
        "# The output will consist of 3 neurons (one per class) with softmax activation\n",
        "# so they represent probabilities\n",
        "outputs = Conv2D(3, (1, 1), activation='softmax') (c7)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJkyiADzuro5"
      },
      "source": [
        "## Training the network\n",
        "Now we are almost ready to train our network! Some important training parameters to take into account:\n",
        "*   `Epochs`: which defines the maximum number of epochs the model will be trained. Initially set to 100.\n",
        "*   `Patience`: number of epochs that produced the monitored quantity (validation Jaccard index) with no improvement after which training will be stopped. Initially set to 50.\n",
        "*   `Batch size`:  the number of training examples in one forward/backward pass. Initially set to 10.\n",
        "*   `Learning rate`:  the parameter that determines the step size to update the weights of the network at each iteration while training the model.\n",
        "\n",
        "\n",
        "Since we have more than 2 output classes, we use the categorical cross-entropy (CCE) between the expected and the predicted pixel values as the loss function, and we also include the Jaccard index as a control metric. The Jaccard index obtained for the validation set during the training is taken into account to define an early stopping schedule for the training. This is important when you want to control the training time. Otherwise, you can wait until the network finishes the training and store the weights of the checkpoint that was doing better in the validation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xycY-tS9F4PN"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Training parameters\n",
        "numEpochs = 100 # maximum number of epochs to train\n",
        "patience = 50   # number of epochs to wait before stopping if no improvement\n",
        "batchSize = 10  # number of samples per batch\n",
        "lr = 0.0003 # learning rate\n",
        "# Define early stopper to finish the training when the network does not improve\n",
        "earlystopper = EarlyStopping(patience=patience, verbose=1, restore_best_weights=True, \n",
        "                             monitor='val_jaccard_index', mode='max')\n",
        "\n",
        "\n",
        "# Finally compile the model with Adam as optimizer, CCE as loss function and IoU\n",
        "# as metric\n",
        "opt = Adam(lr=lr) # Adam with specified learning rate\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[jaccard_index])\n",
        "\n",
        "# Train!\n",
        "history = model.fit( X_train, Y_train, validation_data = (X_val, Y_val),\n",
        "                    batch_size = batchSize, epochs=numEpochs,\n",
        "                    callbacks=[earlystopper])\n",
        "\n",
        "# Save the model weights to and HDF5 file\n",
        "model.save_weights( 'unet_pancreatic_cell_segmentation_best.h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6d_arskwxuP"
      },
      "source": [
        "\n",
        "We can now plot the loss and metric curves for the training and validation sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_VsQDKUGSVS"
      },
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "# Summarize history for loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "# Summarize history for Jaccard index\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['jaccard_index'])\n",
        "plt.plot(history.history['val_jaccard_index'])\n",
        "plt.title('model Jaccard index')\n",
        "plt.ylabel('Jaccard index')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_jacc', 'val_jacc'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>**Exercise**:</font> Play with the batch size and the learning rate. For example, check whether the network learns anything when the learning rate is too high (e.g., 0.01) or whether batch size has any effect on the final result. For this you can reduce the number of epochs so you do not need to wait for long training times. \n"
      ],
      "metadata": {
        "id": "SYXO6kZfb0nO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpMeDioSw406"
      },
      "source": [
        "## Check performance in the test set\n",
        "Finally we can load some test images for testing.\n",
        "\n",
        "<font color='blue'>**Exercise 1**.</font> \n",
        "1. Load all the test images as it was done for the training and validation. Note that you will need to preprocess them as well. Call the variables `X_test` and `Y_test`. Also note that the original images have a size that is not divisible by a factor of 8 so the decoder side of the U-Net cannot create patches of the same shape as the input. Do you think that the network can process something that has a shape different from [256, 256]? how did we make this posssible in the architecture?\n",
        "\n",
        "2. On a paper, try computing the image shape resulting through each of the branches of the U-Net when down-sampling. You should see that if an image has input shape [234,234] for example, you will have some incompatibilities with the skip connections.\n",
        "Generally one would create a tiling strategy to reconstruct the entire images.\n",
        "To make it simpler, we will crop a patch as large as possible that is still divisible by 8.\n",
        "\n",
        "We can evaluate the network performance in test using both the CCE and Jaccar index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOPVk3e2xARB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvVKgUiRL4MJ"
      },
      "source": [
        "# Evaluate trained network on test images\n",
        "\n",
        "# Normalize input images\n",
        "X_test = \n",
        "# One-hot label representation\n",
        "Y_test =\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "print('test loss CCE: {0}, Jaccard index: {1}'.format(results[0], results[1]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vLUfOCxO9U"
      },
      "source": [
        "And also display some patches for qualitative evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o0P2LIGNqYn"
      },
      "source": [
        "print('\\n# Generate predictions for 3 samples')\n",
        "predictions = model.predict(X_test[:1], batch_size=1)\n",
        "masks_prediction = np.array((predictions[0,:,:,0], predictions[0,:,:,1]))\n",
        "contours_prediction = predictions[0,:,:,2]\n",
        "print('predictions shape:', predictions.shape)\n",
        "\n",
        "# Display corresponding first 3 patches\n",
        "plt.figure(figsize=(48,15))\n",
        "plt.subplot(3, 7, 1)\n",
        "plt.imshow( test_input_patches[0], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Test patch at low resolution' )\n",
        "# Side by side with its \"ground truth\"\n",
        "plt.subplot(3, 7, 2)\n",
        "plt.imshow( np.argmax(Y_test[0], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Ground truth' )\n",
        "# One hot final segmentation with argmax\n",
        "plt.subplot(3, 7, 3)\n",
        "plt.imshow(np.argmax(predictions[0], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Final segmentation segmentation' )\n",
        "# Foreground prediction\n",
        "plt.subplot(3, 7, 4)\n",
        "plt.imshow( predictions[0,:,:,1], 'CMRmap')\n",
        "plt.axis('off')\n",
        "plt.title( 'Foreground prediction' )\n",
        "# Contours predictions\n",
        "plt.subplot(3, 7, 5)\n",
        "plt.imshow( predictions[0,:,:,2], 'CMRmap')\n",
        "plt.axis('off')\n",
        "plt.title( 'Contours Prediction' )\n",
        "# ZOOM test ground truth\n",
        "plt.subplot(3, 7, 6)\n",
        "plt.imshow( np.argmax(Y_test[0,100:200, 100:200], axis=-1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Zoomed ground truth' )\n",
        "# ZOOM Ground Truth\n",
        "plt.subplot(3, 7, 7)\n",
        "plt.imshow( np.argmax(predictions[0][100:200,100:200], axis = -1), 'CMRmap', interpolation='nearest')\n",
        "plt.axis('off')\n",
        "plt.title( 'Zoomed prediction' )\n",
        "\n",
        "\n",
        "predictions = model.predict(np.expand_dims(X_test[50], axis = 0), batch_size=1)\n",
        "\n",
        "plt.subplot(3, 7, 8)\n",
        "plt.imshow( test_input_patches[50], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Test patch at low resolution' )\n",
        "# Side by side with its \"ground truth\"\n",
        "plt.subplot(3, 7, 9)\n",
        "plt.imshow( np.argmax(Y_test[50], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Ground truth' )\n",
        "# and one hot final segmentation with argmax\n",
        "plt.subplot(3, 7, 10)\n",
        "plt.imshow(np.argmax(predictions[0], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Final segmentation segmentation' )\n",
        "# foreground prediction\n",
        "plt.subplot(3, 7, 11)\n",
        "plt.imshow( predictions[0,:,:,1], 'CMRmap')\n",
        "plt.axis('off')\n",
        "plt.title( 'Foreground prediction' )\n",
        "# contours predictions\n",
        "plt.subplot(3, 7, 12)\n",
        "plt.imshow( predictions[0,:,:,2], 'CMRmap')\n",
        "plt.axis('off')\n",
        "plt.title( 'Contours Prediction' )\n",
        "# ZOOM test ground truth\n",
        "plt.subplot(3, 7, 13)\n",
        "plt.imshow( np.argmax(Y_test[50,100:200, 100:200], axis=-1), 'CMRmap', interpolation='nearest'  )\n",
        "plt.axis('off')\n",
        "plt.title( 'Zoomed ground truth' )\n",
        "#ZOOM Ground Truth\n",
        "plt.subplot(3, 7, 14)\n",
        "plt.imshow( np.argmax(predictions[0][100:200,100:200], axis = -1), 'CMRmap', interpolation='nearest')\n",
        "plt.axis('off')\n",
        "plt.title( 'Zoomed prediction' )\n",
        "\n",
        "predictions = model.predict(np.expand_dims(X_test[80], axis = 0), batch_size=1)\n",
        "\n",
        "plt.subplot(3, 7, 15)\n",
        "plt.imshow( test_input_patches[80], 'gray' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Test patch at low resolution' )\n",
        "# Side by side with its \"ground truth\"\n",
        "plt.subplot(3, 7, 16)\n",
        "plt.imshow( np.argmax(Y_test[80], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Ground truth' )\n",
        "# and one hot final segmentation with argmax\n",
        "plt.subplot(3, 7, 17)\n",
        "plt.imshow(np.argmax(predictions[0], axis = -1), 'CMRmap', interpolation='nearest' )\n",
        "plt.axis('off')\n",
        "plt.title( 'Final segmentation segmentation' )\n",
        "# foreground prediction\n",
        "plt.subplot(3, 7, 18)\n",
        "plt.imshow( predictions[0,:,:,1], 'CMRmap')\n",
        "plt.axis('off')\n",
        "plt.title( 'Foreground prediction' )\n",
        "# contours predictions\n",
        "plt.subplot(3, 7, 19)\n",
        "plt.imshow( predictions[0,:,:,2], 'CMRmap')\n",
        "plt.axis('off')\n",
        "plt.title( 'Contours Prediction' )\n",
        "# ZOOM test ground truth\n",
        "plt.subplot(3, 7, 20)\n",
        "plt.imshow( np.argmax(Y_test[80,100:200, 100:200], axis=-1), 'CMRmap', interpolation='nearest')\n",
        "plt.axis('off')\n",
        "plt.title( 'Zoomed ground truth' )\n",
        "#ZOOM Ground Truth\n",
        "plt.subplot(3, 7, 21)\n",
        "plt.imshow( np.argmax(predictions[0][100:200,100:200], axis = -1), 'CMRmap', interpolation='nearest')\n",
        "plt.axis('off')\n",
        "plt.title( 'Zoomed prediction' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model's ability to generalise\n",
        "\n",
        "<font color='blue'>**Exercise**:</font> You will take an image from the test set and change the pixel size, for example, by dividing it by 5 or multiplying it by 5. See how the performance of the network changes according these new properties.\n",
        "\n",
        "You can use `cv2.resize(im, dsize=(new_width, new_height), interpolation=cv2.INTER_NEAREST)` to resize your images properly.\n",
        "\n",
        "Display the results. "
      ],
      "metadata": {
        "id": "Yecwcm69YEjg"
      }
    }
  ]
}